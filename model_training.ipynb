{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5AIOAUbnpN1",
        "outputId": "3eef822a-27dd-4290-a410-4cba8f3de820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m337.9/400.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q transformers datasets scikit-learn accelerate optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dJd2Sy4Fvy5R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "HLNWLXxXwL0q"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "class RandomForestTrainer:\n",
        "    def __init__(self, param_grid=None, n_splits=3, random_state=42):\n",
        "        \"\"\"\n",
        "        param_grid: dictionary for hyperparameter tuning\n",
        "        n_splits: number of folds for K-Fold CV\n",
        "        \"\"\"\n",
        "        # Use estimator__ prefix for MultiOutputClassifier\n",
        "        self.param_grid = param_grid if param_grid else {\n",
        "            'estimator__n_estimators': [100, 200],\n",
        "            'estimator__max_depth': [None, 10, 20],\n",
        "            'estimator__min_samples_split': [2, 5]\n",
        "        }\n",
        "        self.n_splits = n_splits\n",
        "        self.random_state = random_state\n",
        "        self.best_model = None\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, save_path=\"rf_model.pkl\"):\n",
        "        \"\"\"\n",
        "        X_train, y_train: training features and labels (multi-label one-hot)\n",
        "        \"\"\"\n",
        "        rf = RandomForestClassifier(random_state=self.random_state, class_weight='balanced_subsample', n_jobs=1)\n",
        "        multi_rf = MultiOutputClassifier(rf, n_jobs=-1)\n",
        "\n",
        "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
        "\n",
        "        random_search = RandomizedSearchCV(\n",
        "            multi_rf,\n",
        "            param_distributions=self.param_grid,\n",
        "            n_iter=1,\n",
        "            cv=kf,\n",
        "            scoring='f1_weighted',\n",
        "            verbose=3,\n",
        "            n_jobs=-1,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        random_search.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
        "\n",
        "        self.best_model = random_search.best_estimator_\n",
        "\n",
        "        if X_val is not None and y_val is not None:\n",
        "            y_pred = self.best_model.predict(X_val)\n",
        "            print(\"Validation Metrics:\")\n",
        "            print(classification_report(y_val, y_pred, zero_division=0))\n",
        "\n",
        "        joblib.dump(self.best_model, save_path)\n",
        "        print(f\"Final model saved to {save_path}\")\n",
        "\n",
        "        return self.best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zFA20JX9x2QK"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"./train_fp1024.csv\")\n",
        "val_df   = pd.read_csv(\"./valid_fp1024.csv\")\n",
        "test_df  = pd.read_csv(\"./test_fp1024.csv\")\n",
        "\n",
        "label_cols = train_df.columns[1:13]\n",
        "feature_cols = train_df.columns[13:]\n",
        "\n",
        "y_train = train_df[label_cols].values\n",
        "y_val   = val_df[label_cols].values\n",
        "y_test  = test_df[label_cols].values\n",
        "\n",
        "X_train = train_df[feature_cols].values\n",
        "X_val   = val_df[feature_cols].values\n",
        "X_test  = test_df[feature_cols].values\n",
        "\n",
        "param_grid = {\n",
        "    'estimator__n_estimators': [400],\n",
        "    'estimator__max_depth': [40],\n",
        "    'estimator__min_samples_split': [5],\n",
        "    'estimator__min_samples_leaf': [1],\n",
        "    'estimator__max_features': ['sqrt'],\n",
        "    'estimator__class_weight': ['balanced_subsample']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYpWtNuY3h6N",
        "outputId": "19bd22cc-e5f4-4556-f57a-2e1870f11a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'estimator__n_estimators': 400, 'estimator__min_samples_split': 5, 'estimator__min_samples_leaf': 1, 'estimator__max_features': 'sqrt', 'estimator__max_depth': 40, 'estimator__class_weight': 'balanced_subsample'}\n",
            "Validation Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.45      0.58        31\n",
            "           1       0.65      0.44      0.52        25\n",
            "           2       0.64      0.18      0.29        87\n",
            "           3       1.00      0.11      0.20        45\n",
            "           4       0.57      0.17      0.27        75\n",
            "           5       0.73      0.28      0.40        29\n",
            "           6       0.00      0.00      0.00        32\n",
            "           7       0.67      0.09      0.17       106\n",
            "           8       0.00      0.00      0.00        35\n",
            "           9       0.67      0.05      0.09        44\n",
            "          10       0.56      0.09      0.16       111\n",
            "          11       0.67      0.08      0.14        75\n",
            "\n",
            "   micro avg       0.65      0.14      0.23       695\n",
            "   macro avg       0.58      0.16      0.23       695\n",
            "weighted avg       0.60      0.14      0.21       695\n",
            " samples avg       0.06      0.04      0.05       695\n",
            "\n",
            "Final model saved to toxicity_rf_model.pkl\n"
          ]
        }
      ],
      "source": [
        "rf_trainer = RandomForestTrainer(param_grid=param_grid, n_splits=3)\n",
        "rf_model = rf_trainer.train(X_train, y_train, X_val, y_val, save_path=\"toxicity_rf_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "vkFbYu6n3s89"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "\n",
        "class LightGBMTrainer:\n",
        "    def __init__(self, param_grid=None, n_iter=20, cv=3, scoring=\"f1_macro\", random_state=42, n_jobs=-1, verbose=2):\n",
        "        \"\"\"\n",
        "        Multi-label LightGBM trainer using MultiOutputClassifier + RandomizedSearchCV.\n",
        "        \"\"\"\n",
        "\n",
        "        self.base_estimator = LGBMClassifier(\n",
        "            objective=\"binary\",\n",
        "            boosting_type=\"gbdt\",\n",
        "            random_state=random_state,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        self.model = MultiOutputClassifier(self.base_estimator, n_jobs=-1)\n",
        "\n",
        "        if param_grid is None:\n",
        "            self.param_grid = {\n",
        "                \"estimator__num_leaves\": [63],\n",
        "                \"estimator__n_estimators\": [400],\n",
        "                \"estimator__learning_rate\": [0.1, 0.05, 0.01],\n",
        "                \"estimator__max_depth\": [15],\n",
        "                \"estimator__min_child_samples\": [50],\n",
        "                \"estimator__subsample\": [0.9],\n",
        "            }\n",
        "        else:\n",
        "            self.param_grid = param_grid\n",
        "\n",
        "        self.n_iter = n_iter\n",
        "        self.cv = cv\n",
        "        self.scoring = scoring\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "        self.verbose = verbose\n",
        "        self.search = None\n",
        "        self.best_model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit model with hyperparameter search.\n",
        "        \"\"\"\n",
        "        self.search = RandomizedSearchCV(\n",
        "            self.model,\n",
        "            param_distributions=self.param_grid,\n",
        "            n_iter=self.n_iter,\n",
        "            cv=self.cv,\n",
        "            scoring=self.scoring,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=self.n_jobs,\n",
        "            verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        self.search.fit(X, y)\n",
        "        self.best_model = self.search.best_estimator_\n",
        "        return self.best_model\n",
        "\n",
        "    def evaluate(self, X, y, label_names=None):\n",
        "        \"\"\"\n",
        "        Evaluate model on test/validation data.\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"Model not trained yet. Call fit() first.\")\n",
        "\n",
        "        y_pred = self.best_model.predict(X)\n",
        "\n",
        "        report = classification_report(y, y_pred, target_names=label_names, zero_division=0)\n",
        "        macro_f1 = f1_score(y, y_pred, average=\"macro\", zero_division=0)\n",
        "        micro_f1 = f1_score(y, y_pred, average=\"micro\", zero_division=0)\n",
        "\n",
        "        return {\n",
        "            \"report\": report,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"micro_f1\": micro_f1\n",
        "        }\n",
        "\n",
        "    def save(self, path=\"lgbm_multioutput.pkl\"):\n",
        "        \"\"\"\n",
        "        Save trained model to disk.\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No trained model to save.\")\n",
        "        joblib.dump(self.best_model, path)\n",
        "\n",
        "    def load(self, path=\"lgbm_multioutput.pkl\"):\n",
        "        \"\"\"\n",
        "        Load trained model from disk.\n",
        "        \"\"\"\n",
        "        self.best_model = joblib.load(path)\n",
        "        return self.best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7tfUbx8Tgqu",
        "outputId": "d885486c-a149-4927-cb79-4c8f362432bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=20. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "        NR-AR       0.58      0.26      0.36        27\n",
            "    NR-AR-LBD       0.56      0.26      0.36        19\n",
            "       NR-AhR       0.36      0.23      0.28        92\n",
            " NR-Aromatase       0.45      0.11      0.17        47\n",
            "        NR-ER       0.44      0.11      0.18        70\n",
            "    NR-ER-LBD       0.22      0.10      0.13        21\n",
            "NR-PPAR-gamma       0.00      0.00      0.00        22\n",
            "       SR-ARE       0.26      0.04      0.07       118\n",
            "     SR-ATAD5       0.17      0.03      0.05        33\n",
            "       SR-HSE       0.67      0.09      0.15        47\n",
            "       SR-MMP       0.40      0.10      0.17        96\n",
            "       SR-p53       0.78      0.10      0.17        72\n",
            "\n",
            "    micro avg       0.41      0.11      0.18       664\n",
            "    macro avg       0.41      0.12      0.17       664\n",
            " weighted avg       0.42      0.11      0.17       664\n",
            "  samples avg       0.06      0.04      0.04       664\n",
            "\n",
            "Macro F1: 0.17459792752883316\n",
            "Micro F1: 0.1770956316410862\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"./train_fp1024.csv\")\n",
        "val_df   = pd.read_csv(\"./valid_fp1024.csv\")\n",
        "test_df  = pd.read_csv(\"./test_fp1024.csv\")\n",
        "\n",
        "label_cols = train_df.columns[1:13]\n",
        "feature_cols = train_df.columns[13:]\n",
        "\n",
        "X_train, y_train = train_df[feature_cols].values, train_df[label_cols].values\n",
        "X_val,   y_val   = val_df[feature_cols].values, val_df[label_cols].values\n",
        "X_test,  y_test  = test_df[feature_cols].values, test_df[label_cols].values\n",
        "\n",
        "\n",
        "trainer = LightGBMTrainer(n_iter=20, cv=3, n_jobs=-1)\n",
        "\n",
        "# Train\n",
        "trainer.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "results = trainer.evaluate(X_test, y_test, label_names=label_cols)\n",
        "print(results[\"report\"])\n",
        "print(\"Macro F1:\", results[\"macro_f1\"])\n",
        "print(\"Micro F1:\", results[\"micro_f1\"])\n",
        "\n",
        "# Save\n",
        "trainer.save(\"tox21_lightgbm.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "73ge-6cxTvyT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, f1_score\n",
        "import joblib\n",
        "\n",
        "class LightGBMTrainer:\n",
        "    def __init__(self, params=None, num_boost_round=500, early_stopping_rounds=50, random_state=42):\n",
        "        \"\"\"\n",
        "        params: dictionary of LightGBM hyperparameters\n",
        "        num_boost_round: max boosting rounds\n",
        "        early_stopping_rounds: early stopping for validation\n",
        "        \"\"\"\n",
        "        self.params = params if params else {\n",
        "            'objective': 'binary',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': 31,\n",
        "            'learning_rate': 0.05,\n",
        "            'is_unbalance': True,\n",
        "            'metric': 'binary_logloss',\n",
        "            'random_state': random_state,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        self.num_boost_round = num_boost_round\n",
        "        self.early_stopping_rounds = early_stopping_rounds\n",
        "        self.models = []\n",
        "        self.thresholds = []\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, save_path=\"lgbm_model.pkl\"):\n",
        "        \"\"\"\n",
        "        X_train, y_train: training features and labels (multi-label one-hot)\n",
        "        X_val, y_val: optional validation set for metrics\n",
        "        \"\"\"\n",
        "        n_tasks = y_train.shape[1]\n",
        "        self.models = []\n",
        "        self.thresholds = []\n",
        "\n",
        "        for task_idx in range(n_tasks):\n",
        "            print(f\"\\nTraining task {task_idx+1}/{n_tasks}\")\n",
        "            train_data = lgb.Dataset(X_train, label=y_train[:, task_idx])\n",
        "\n",
        "            valid_data = lgb.Dataset(X_val, label=y_val[:, task_idx]) if X_val is not None else None\n",
        "\n",
        "            callbacks = []\n",
        "            if X_val is not None and y_val is not None:\n",
        "              callbacks.append(lgb.early_stopping(stopping_rounds=self.early_stopping_rounds, verbose=True))\n",
        "\n",
        "            model = lgb.train(\n",
        "              self.params,\n",
        "              train_data,\n",
        "              num_boost_round=self.num_boost_round,\n",
        "              valid_sets=[valid_data] if valid_data is not None else None,\n",
        "              callbacks=callbacks\n",
        "            )\n",
        "            self.models.append(model)\n",
        "\n",
        "            # threshold tuning on validation set\n",
        "            if X_val is not None and y_val is not None:\n",
        "                probs = model.predict(X_val)\n",
        "                precision, recall, thres = precision_recall_curve(y_val[:, task_idx], probs)\n",
        "                f1 = 2*precision*recall/(precision+recall+1e-6)\n",
        "                best_thres = thres[f1.argmax()] if len(thres) > 0 else 0.5\n",
        "                self.thresholds.append(best_thres)\n",
        "            else:\n",
        "                self.thresholds.append(0.5)\n",
        "\n",
        "        joblib.dump({'models': self.models, 'thresholds': self.thresholds}, save_path)\n",
        "        print(f\"\\nAll models saved to {save_path}\")\n",
        "\n",
        "        if X_val is not None and y_val is not None:\n",
        "            y_pred = self.predict(X_val)\n",
        "            print(\"\\nValidation Metrics:\")\n",
        "            print(classification_report(y_val, y_pred, zero_division=0))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Returns predictions using tuned thresholds\n",
        "        \"\"\"\n",
        "        n_tasks = len(self.models)\n",
        "        y_pred = np.zeros((X.shape[0], n_tasks))\n",
        "\n",
        "        for i, model in enumerate(self.models):\n",
        "            probs = model.predict(X)\n",
        "            y_pred[:, i] = (probs >= self.thresholds[i]).astype(int)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Returns predicted probabilities for each task\n",
        "        \"\"\"\n",
        "        n_tasks = len(self.models)\n",
        "        y_proba = np.zeros((X.shape[0], n_tasks))\n",
        "\n",
        "        for i, model in enumerate(self.models):\n",
        "            y_proba[:, i] = model.predict(X)\n",
        "\n",
        "        return y_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAkE6SaXZcpV",
        "outputId": "2b776f84-ae46-4c9c-d144-8acb80a2327c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training task 1/12\n",
            "[LightGBM] [Info] Number of positive: 250, number of negative: 6008\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043534 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039949 -> initscore=-3.179386\n",
            "[LightGBM] [Info] Start training from score -3.179386\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.162613\n",
            "\n",
            "Training task 2/12\n",
            "[LightGBM] [Info] Number of positive: 193, number of negative: 6065\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030841 -> initscore=-3.447600\n",
            "[LightGBM] [Info] Start training from score -3.447600\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.136444\n",
            "\n",
            "Training task 3/12\n",
            "[LightGBM] [Info] Number of positive: 589, number of negative: 5669\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043993 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094120 -> initscore=-2.264342\n",
            "[LightGBM] [Info] Start training from score -2.264342\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.336145\n",
            "\n",
            "Training task 4/12\n",
            "[LightGBM] [Info] Number of positive: 208, number of negative: 6050\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044998 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033237 -> initscore=-3.370275\n",
            "[LightGBM] [Info] Start training from score -3.370275\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.215597\n",
            "\n",
            "Training task 5/12\n",
            "[LightGBM] [Info] Number of positive: 646, number of negative: 5612\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048691 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103228 -> initscore=-2.161863\n",
            "[LightGBM] [Info] Start training from score -2.161863\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.316331\n",
            "\n",
            "Training task 6/12\n",
            "[LightGBM] [Info] Number of positive: 299, number of negative: 5959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043956 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047779 -> initscore=-2.992214\n",
            "[LightGBM] [Info] Start training from score -2.992214\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.161129\n",
            "\n",
            "Training task 7/12\n",
            "[LightGBM] [Info] Number of positive: 132, number of negative: 6126\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021093 -> initscore=-3.837495\n",
            "[LightGBM] [Info] Start training from score -3.837495\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.176042\n",
            "\n",
            "Training task 8/12\n",
            "[LightGBM] [Info] Number of positive: 718, number of negative: 5540\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114733 -> initscore=-2.043280\n",
            "[LightGBM] [Info] Start training from score -2.043280\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's binary_logloss: 0.392083\n",
            "\n",
            "Training task 9/12\n",
            "[LightGBM] [Info] Number of positive: 196, number of negative: 6062\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044855 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031320 -> initscore=-3.431680\n",
            "[LightGBM] [Info] Start training from score -3.431680\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.185848\n",
            "\n",
            "Training task 10/12\n",
            "[LightGBM] [Info] Number of positive: 281, number of negative: 5977\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044457 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.044903 -> initscore=-3.057319\n",
            "[LightGBM] [Info] Start training from score -3.057319\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0.213213\n",
            "\n",
            "Training task 11/12\n",
            "[LightGBM] [Info] Number of positive: 711, number of negative: 5547\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.113615 -> initscore=-2.054340\n",
            "[LightGBM] [Info] Start training from score -2.054340\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's binary_logloss: 0.390159\n",
            "\n",
            "Training task 12/12\n",
            "[LightGBM] [Info] Number of positive: 276, number of negative: 5982\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044098 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2004\n",
            "[LightGBM] [Info] Number of data points in the train set: 6258, number of used features: 1002\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.044104 -> initscore=-3.076109\n",
            "[LightGBM] [Info] Start training from score -3.076109\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's binary_logloss: 0.305976\n",
            "\n",
            "All models saved to toxicity_lgbm.pkl\n",
            "\n",
            "Validation Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.48      0.43        31\n",
            "           1       0.26      0.44      0.33        25\n",
            "           2       0.44      0.28      0.34        87\n",
            "           3       0.20      0.24      0.22        45\n",
            "           4       0.25      0.17      0.21        75\n",
            "           5       0.44      0.24      0.31        29\n",
            "           6       0.08      0.47      0.13        32\n",
            "           7       0.18      0.70      0.28       106\n",
            "           8       0.06      0.51      0.11        35\n",
            "           9       0.10      0.43      0.16        44\n",
            "          10       0.28      0.49      0.36       111\n",
            "          11       0.17      0.55      0.26        75\n",
            "\n",
            "   micro avg       0.17      0.43      0.24       695\n",
            "   macro avg       0.24      0.42      0.26       695\n",
            "weighted avg       0.24      0.43      0.27       695\n",
            " samples avg       0.13      0.18      0.13       695\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.37      0.31        27\n",
            "           1       0.16      0.32      0.21        19\n",
            "           2       0.24      0.14      0.18        92\n",
            "           3       0.22      0.26      0.24        47\n",
            "           4       0.16      0.10      0.12        70\n",
            "           5       0.07      0.05      0.06        21\n",
            "           6       0.04      0.32      0.07        22\n",
            "           7       0.16      0.55      0.25       118\n",
            "           8       0.07      0.64      0.12        33\n",
            "           9       0.09      0.36      0.14        47\n",
            "          10       0.21      0.45      0.29        96\n",
            "          11       0.16      0.50      0.25        72\n",
            "\n",
            "   micro avg       0.14      0.36      0.20       664\n",
            "   macro avg       0.15      0.34      0.19       664\n",
            "weighted avg       0.17      0.36      0.21       664\n",
            " samples avg       0.10      0.15      0.10       664\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load your data\n",
        "train_df = pd.read_csv(\"./train_fp1024.csv\")\n",
        "val_df   = pd.read_csv(\"./valid_fp1024.csv\")\n",
        "test_df  = pd.read_csv(\"./test_fp1024.csv\")\n",
        "\n",
        "label_cols = train_df.columns[1:13]\n",
        "feature_cols = train_df.columns[13:]\n",
        "\n",
        "X_train, y_train = train_df[feature_cols].values, train_df[label_cols].values\n",
        "X_val, y_val     = val_df[feature_cols].values, val_df[label_cols].values\n",
        "X_test, y_test   = test_df[feature_cols].values, test_df[label_cols].values\n",
        "\n",
        "# Initialize and train\n",
        "lgb_trainer = LightGBMTrainer(num_boost_round=1000, early_stopping_rounds=50)\n",
        "lgb_trainer.train(X_train, y_train, X_val, y_val, save_path=\"toxicity_lgbm.pkl\")\n",
        "\n",
        "# Test set predictions\n",
        "y_test_pred = lgb_trainer.predict(X_test)\n",
        "y_test_proba = lgb_trainer.predict_proba(X_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_test_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P7CKS74EZf1r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "import optuna\n",
        "\n",
        "class ESOLTrainer:\n",
        "    def __init__(self, data_dir=\".\", target_col=\"measured log solubility in mols per litre\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_col = target_col\n",
        "        self.models = {}\n",
        "        self.best_params = None\n",
        "\n",
        "        # Load data\n",
        "        self.train = pd.read_csv(os.path.join(data_dir, \"train_with_desc.csv\"))\n",
        "        self.valid = pd.read_csv(os.path.join(data_dir, \"valid_with_desc.csv\"))\n",
        "        self.test = pd.read_csv(os.path.join(data_dir, \"test_with_desc.csv\"))\n",
        "\n",
        "        # Split X, y\n",
        "        self.X_train, self.y_train = self._split_xy(self.train)\n",
        "        self.X_valid, self.y_valid = self._split_xy(self.valid)\n",
        "        self.X_test, self.y_test = self._split_xy(self.test)\n",
        "\n",
        "    def _split_xy(self, df):\n",
        "        X = df.drop(columns=[self.target_col, \"smiles\"], errors=\"ignore\")\n",
        "        y = df[self.target_col]\n",
        "        return X, y\n",
        "\n",
        "    def _objective(self, trial):\n",
        "        params = {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": \"rmse\",\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.05, log=True),\n",
        "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
        "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 50),\n",
        "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
        "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
        "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
        "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
        "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
        "            \"verbosity\": -1,\n",
        "            \"seed\": 42,\n",
        "        }\n",
        "\n",
        "        dtrain = lgb.Dataset(self.X_train, self.y_train)\n",
        "        dvalid = lgb.Dataset(self.X_valid, self.y_valid, reference=dtrain)\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            valid_sets=[dvalid],\n",
        "            num_boost_round=5000,\n",
        "            callbacks=[early_stopping(100), log_evaluation(200)]\n",
        "        )\n",
        "\n",
        "        preds = model.predict(self.X_valid, num_iteration=model.best_iteration)\n",
        "        rmse = mean_squared_error(self.y_valid, preds)\n",
        "        return rmse\n",
        "\n",
        "    def tune(self, n_trials=30):\n",
        "        \"\"\"Run Optuna tuning and save best params.\"\"\"\n",
        "        study = optuna.create_study(direction=\"minimize\")\n",
        "        study.optimize(self._objective, n_trials=n_trials)\n",
        "        self.best_params = study.best_params\n",
        "        print(\"Best params:\", self.best_params)\n",
        "        return self.best_params\n",
        "\n",
        "    def trainlgb(self, tuned=True):\n",
        "        \"\"\"Train LightGBM with either default or tuned params.\"\"\"\n",
        "        if tuned and self.best_params is None:\n",
        "            print(\"⚠️ No tuned params found, running tuning first...\")\n",
        "            self.tune(n_trials=30)\n",
        "\n",
        "        params = self.best_params if (tuned and self.best_params) else {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": \"rmse\",\n",
        "            \"boosting_type\": \"gbdt\",\n",
        "            \"learning_rate\": 0.01,\n",
        "            \"num_leaves\": 64,\n",
        "            \"feature_fraction\": 0.9,\n",
        "            \"bagging_fraction\": 0.8,\n",
        "            \"bagging_freq\": 5,\n",
        "            \"seed\": 42,\n",
        "        }\n",
        "\n",
        "        dtrain = lgb.Dataset(self.X_train, self.y_train)\n",
        "        dvalid = lgb.Dataset(self.X_valid, self.y_valid, reference=dtrain)\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            valid_sets=[dvalid],\n",
        "            num_boost_round=5000,\n",
        "            callbacks=[early_stopping(100), log_evaluation(200)]\n",
        "        )\n",
        "\n",
        "        self.models[\"final\"] = model\n",
        "\n",
        "        # Validation metrics\n",
        "        val_preds = model.predict(self.X_valid, num_iteration=model.best_iteration)\n",
        "        val_rmse = mean_squared_error(self.y_valid, val_preds)\n",
        "        val_r2 = r2_score(self.y_valid, val_preds)\n",
        "        print(f\"✅ Validation RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
        "\n",
        "        # Test metrics\n",
        "        test_preds = model.predict(self.X_test, num_iteration=model.best_iteration)\n",
        "        test_rmse = mean_squared_error(self.y_test, test_preds)\n",
        "        test_r2 = r2_score(self.y_test, test_preds)\n",
        "        print(f\"✅ Test RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
        "\n",
        "        return model, (val_rmse, val_r2), (test_rmse, test_r2)\n",
        "\n",
        "    def save_model(self, filepath=\"esol_model.txt\"):\n",
        "        \"\"\"Save the final trained model to disk.\"\"\"\n",
        "        if \"final\" not in self.models:\n",
        "            raise ValueError(\"No trained model found. Train a model first with trainlgb().\")\n",
        "        self.models[\"final\"].save_model(filepath)\n",
        "        print(f\"💾 Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath=\"esol_model.txt\"):\n",
        "        \"\"\"Load a trained model from disk into self.models['final'].\"\"\"\n",
        "        self.models[\"final\"] = lgb.Booster(model_file=filepath)\n",
        "        print(f\"📂 Model loaded from {filepath}\")\n",
        "        return self.models[\"final\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "93AG2hAOv_tR",
        "outputId": "bbaf3759-ccf3-4c7d-ccc4-cf0898d9d395"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:33:57,689] A new study created in memory with name: no-name-150fdbda-4613-44a6-8ab7-2a67db3d5cc9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ No tuned params found, running tuning first...\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:33:58,496] Trial 0 finished with value: 0.05925424002053358 and parameters: {'learning_rate': 0.04080030836352462, 'num_leaves': 173, 'max_depth': 7, 'min_data_in_leaf': 16, 'feature_fraction': 0.7307288480795568, 'bagging_fraction': 0.6273036457175115, 'bagging_freq': 8, 'lambda_l1': 1.754166876021954e-08, 'lambda_l2': 0.08323975954686595}. Best is trial 0 with value: 0.05925424002053358.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[93]\tvalid_0's rmse: 0.243422\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.239279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:00,485] Trial 1 finished with value: 0.05710865064934991 and parameters: {'learning_rate': 0.02632747127126337, 'num_leaves': 69, 'max_depth': 11, 'min_data_in_leaf': 30, 'feature_fraction': 0.6911060563298117, 'bagging_fraction': 0.8784526526603424, 'bagging_freq': 1, 'lambda_l1': 6.284803768963424e-07, 'lambda_l2': 0.0780632710071345}. Best is trial 1 with value: 0.05710865064934991.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[195]\tvalid_0's rmse: 0.238974\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:02,379] Trial 2 finished with value: 0.056346433971335444 and parameters: {'learning_rate': 0.035777803614116054, 'num_leaves': 94, 'max_depth': 11, 'min_data_in_leaf': 17, 'feature_fraction': 0.6160799647639202, 'bagging_fraction': 0.7777987107983574, 'bagging_freq': 1, 'lambda_l1': 0.0006456961429973911, 'lambda_l2': 1.8128993884784538e-07}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[200]\tvalid_0's rmse: 0.238573\n",
            "Early stopping, best iteration is:\n",
            "[113]\tvalid_0's rmse: 0.237374\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.287993\n",
            "[400]\tvalid_0's rmse: 0.247328\n",
            "[600]\tvalid_0's rmse: 0.243183\n",
            "[800]\tvalid_0's rmse: 0.241516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:07,570] Trial 3 finished with value: 0.058287874323639834 and parameters: {'learning_rate': 0.007328991971123967, 'num_leaves': 91, 'max_depth': 10, 'min_data_in_leaf': 27, 'feature_fraction': 0.8164665806274516, 'bagging_fraction': 0.9347553791684166, 'bagging_freq': 5, 'lambda_l1': 1.0560147930376167e-08, 'lambda_l2': 4.239179303635072e-05}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[826]\tvalid_0's rmse: 0.241429\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.244087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:08,891] Trial 4 finished with value: 0.058211291268631456 and parameters: {'learning_rate': 0.04566364939947758, 'num_leaves': 245, 'max_depth': 11, 'min_data_in_leaf': 39, 'feature_fraction': 0.8278290453796879, 'bagging_fraction': 0.92565676739069, 'bagging_freq': 8, 'lambda_l1': 0.04996389870521953, 'lambda_l2': 0.22531692348880805}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[143]\tvalid_0's rmse: 0.24127\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.358573\n",
            "[400]\tvalid_0's rmse: 0.278792\n",
            "[600]\tvalid_0's rmse: 0.25417\n",
            "[800]\tvalid_0's rmse: 0.246773\n",
            "[1000]\tvalid_0's rmse: 0.242536\n",
            "[1200]\tvalid_0's rmse: 0.241573\n",
            "[1400]\tvalid_0's rmse: 0.239911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:15,294] Trial 5 finished with value: 0.0572975130497897 and parameters: {'learning_rate': 0.005078082657210433, 'num_leaves': 90, 'max_depth': 9, 'min_data_in_leaf': 43, 'feature_fraction': 0.9162516960654926, 'bagging_fraction': 0.8499455791926723, 'bagging_freq': 5, 'lambda_l1': 2.4617839316772487, 'lambda_l2': 0.0003720519138324713}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[1485]\tvalid_0's rmse: 0.239369\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.254451\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:16,453] Trial 6 finished with value: 0.06246523460494778 and parameters: {'learning_rate': 0.020950273678173273, 'num_leaves': 128, 'max_depth': 8, 'min_data_in_leaf': 16, 'feature_fraction': 0.7879221982530066, 'bagging_fraction': 0.7450916929873377, 'bagging_freq': 10, 'lambda_l1': 4.680010791485529, 'lambda_l2': 0.002051145497965467}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[400]\tvalid_0's rmse: 0.250691\n",
            "Early stopping, best iteration is:\n",
            "[370]\tvalid_0's rmse: 0.24993\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.275368\n",
            "[400]\tvalid_0's rmse: 0.243373\n",
            "[600]\tvalid_0's rmse: 0.239822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:21,170] Trial 7 finished with value: 0.057430193024359734 and parameters: {'learning_rate': 0.008689636154250885, 'num_leaves': 167, 'max_depth': 15, 'min_data_in_leaf': 29, 'feature_fraction': 0.7074492925925563, 'bagging_fraction': 0.9388441516425692, 'bagging_freq': 5, 'lambda_l1': 3.253409233879348e-05, 'lambda_l2': 9.948747791291135e-07}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[603]\tvalid_0's rmse: 0.239646\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.248378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:21,908] Trial 8 finished with value: 0.0605527024753598 and parameters: {'learning_rate': 0.022247594710456762, 'num_leaves': 49, 'max_depth': 3, 'min_data_in_leaf': 31, 'feature_fraction': 0.8152896070158933, 'bagging_fraction': 0.8265503788105535, 'bagging_freq': 10, 'lambda_l1': 2.5233538393738657, 'lambda_l2': 0.009388687811896074}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[400]\tvalid_0's rmse: 0.247358\n",
            "Early stopping, best iteration is:\n",
            "[360]\tvalid_0's rmse: 0.246075\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.264314\n",
            "[400]\tvalid_0's rmse: 0.253735\n",
            "[600]\tvalid_0's rmse: 0.251901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:23,224] Trial 9 finished with value: 0.06328645004679091 and parameters: {'learning_rate': 0.02010271891872505, 'num_leaves': 88, 'max_depth': 9, 'min_data_in_leaf': 20, 'feature_fraction': 0.6870819999877861, 'bagging_fraction': 0.6164312578448112, 'bagging_freq': 3, 'lambda_l1': 5.251621291255731, 'lambda_l2': 0.0020118568371399193}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[666]\tvalid_0's rmse: 0.251568\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.253598\n",
            "[400]\tvalid_0's rmse: 0.244234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:26,947] Trial 10 finished with value: 0.059556791713176724 and parameters: {'learning_rate': 0.012648193890219523, 'num_leaves': 26, 'max_depth': 14, 'min_data_in_leaf': 5, 'feature_fraction': 0.6128095211674864, 'bagging_fraction': 0.7473033622887405, 'bagging_freq': 1, 'lambda_l1': 0.0015477779102656326, 'lambda_l2': 1.778076135869884e-08}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[332]\tvalid_0's rmse: 0.244043\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.238816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:29,722] Trial 11 finished with value: 0.057032992410974405 and parameters: {'learning_rate': 0.03216297199416629, 'num_leaves': 50, 'max_depth': 12, 'min_data_in_leaf': 6, 'feature_fraction': 0.6019897529892858, 'bagging_fraction': 0.7374859501373847, 'bagging_freq': 1, 'lambda_l1': 6.138328113928863e-06, 'lambda_l2': 6.596922622499779}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[200]\tvalid_0's rmse: 0.238816\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.24621\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:30,819] Trial 12 finished with value: 0.05990185728055453 and parameters: {'learning_rate': 0.03360338390225604, 'num_leaves': 16, 'max_depth': 13, 'min_data_in_leaf': 5, 'feature_fraction': 0.6165047601430819, 'bagging_fraction': 0.7260253123059303, 'bagging_freq': 3, 'lambda_l1': 3.440669413702188e-05, 'lambda_l2': 4.315686736567804e-06}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[178]\tvalid_0's rmse: 0.244749\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.238508\n",
            "[400]\tvalid_0's rmse: 0.238732\n",
            "Early stopping, best iteration is:\n",
            "[322]\tvalid_0's rmse: 0.237412\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:34,156] Trial 13 finished with value: 0.05636461664350138 and parameters: {'learning_rate': 0.03120244658140354, 'num_leaves': 124, 'max_depth': 12, 'min_data_in_leaf': 11, 'feature_fraction': 0.6091706170230895, 'bagging_fraction': 0.6838226628494538, 'bagging_freq': 3, 'lambda_l1': 0.000979502819910046, 'lambda_l2': 5.800327598994665}. Best is trial 2 with value: 0.056346433971335444.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.24557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:36,366] Trial 14 finished with value: 0.05591230701332577 and parameters: {'learning_rate': 0.013365775645645738, 'num_leaves': 135, 'max_depth': 4, 'min_data_in_leaf': 16, 'feature_fraction': 0.9982114685027492, 'bagging_fraction': 0.6821130233464215, 'bagging_freq': 3, 'lambda_l1': 0.0024075683948095923, 'lambda_l2': 2.864623228001242e-08}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[400]\tvalid_0's rmse: 0.238077\n",
            "Early stopping, best iteration is:\n",
            "[333]\tvalid_0's rmse: 0.236458\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.248624\n",
            "[400]\tvalid_0's rmse: 0.238986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:38,435] Trial 15 finished with value: 0.05669000568989172 and parameters: {'learning_rate': 0.012829084653025971, 'num_leaves': 180, 'max_depth': 5, 'min_data_in_leaf': 22, 'feature_fraction': 0.9860854552334561, 'bagging_fraction': 0.672263460512831, 'bagging_freq': 3, 'lambda_l1': 0.017548117191260294, 'lambda_l2': 3.811631905324517e-08}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[600]\tvalid_0's rmse: 0.238893\n",
            "Early stopping, best iteration is:\n",
            "[537]\tvalid_0's rmse: 0.238097\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.246366\n",
            "[400]\tvalid_0's rmse: 0.242117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:39,991] Trial 16 finished with value: 0.058401144141127175 and parameters: {'learning_rate': 0.01493374875148112, 'num_leaves': 218, 'max_depth': 6, 'min_data_in_leaf': 50, 'feature_fraction': 0.8881240169180005, 'bagging_fraction': 0.792041891559476, 'bagging_freq': 2, 'lambda_l1': 0.033947184192330046, 'lambda_l2': 3.8719707107812205e-07}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[391]\tvalid_0's rmse: 0.241663\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.270887\n",
            "[400]\tvalid_0's rmse: 0.242645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:41,182] Trial 17 finished with value: 0.0583643306848945 and parameters: {'learning_rate': 0.009650496315969119, 'num_leaves': 146, 'max_depth': 3, 'min_data_in_leaf': 13, 'feature_fraction': 0.9932082312124646, 'bagging_fraction': 0.7810843217448361, 'bagging_freq': 4, 'lambda_l1': 0.004744103773913055, 'lambda_l2': 1.6645024034376588e-05}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[459]\tvalid_0's rmse: 0.241587\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.240797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:42,209] Trial 18 finished with value: 0.056789356381044855 and parameters: {'learning_rate': 0.0173793519150575, 'num_leaves': 106, 'max_depth': 5, 'min_data_in_leaf': 23, 'feature_fraction': 0.9218898321666922, 'bagging_fraction': 0.6779344372104937, 'bagging_freq': 2, 'lambda_l1': 0.00021311427686568192, 'lambda_l2': 1.5302874996802269e-07}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[228]\tvalid_0's rmse: 0.238305\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.31424\n",
            "[400]\tvalid_0's rmse: 0.254027\n",
            "[600]\tvalid_0's rmse: 0.2434\n",
            "[800]\tvalid_0's rmse: 0.240851\n",
            "[1000]\tvalid_0's rmse: 0.240544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:49,850] Trial 19 finished with value: 0.057752588978930534 and parameters: {'learning_rate': 0.00581517552164164, 'num_leaves': 198, 'max_depth': 7, 'min_data_in_leaf': 11, 'feature_fraction': 0.7740421912884066, 'bagging_fraction': 0.985939045942724, 'bagging_freq': 7, 'lambda_l1': 0.2445240815117757, 'lambda_l2': 1.2405435254432184e-08}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[1036]\tvalid_0's rmse: 0.240318\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.257655\n",
            "[400]\tvalid_0's rmse: 0.240717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:50,995] Trial 20 finished with value: 0.05789424759769522 and parameters: {'learning_rate': 0.011184336483613746, 'num_leaves': 145, 'max_depth': 4, 'min_data_in_leaf': 34, 'feature_fraction': 0.8718730808226087, 'bagging_fraction': 0.6470450548001623, 'bagging_freq': 2, 'lambda_l1': 0.00016760165717659355, 'lambda_l2': 1.6174784995172909e-06}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[446]\tvalid_0's rmse: 0.240612\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.241957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:53,933] Trial 21 finished with value: 0.05801615189461197 and parameters: {'learning_rate': 0.02578755317259994, 'num_leaves': 123, 'max_depth': 12, 'min_data_in_leaf': 10, 'feature_fraction': 0.6488199473799976, 'bagging_fraction': 0.6897081035840184, 'bagging_freq': 4, 'lambda_l1': 0.002187169492822042, 'lambda_l2': 1.0112447314005157e-07}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[224]\tvalid_0's rmse: 0.240865\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.244526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:55,846] Trial 22 finished with value: 0.059644103554850336 and parameters: {'learning_rate': 0.037057978071431304, 'num_leaves': 114, 'max_depth': 13, 'min_data_in_leaf': 17, 'feature_fraction': 0.6448837526487453, 'bagging_fraction': 0.7092964312151666, 'bagging_freq': 4, 'lambda_l1': 0.0005041160169338643, 'lambda_l2': 8.203872240101532}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[185]\tvalid_0's rmse: 0.244221\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.245218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:34:59,626] Trial 23 finished with value: 0.05930775160941522 and parameters: {'learning_rate': 0.047690506893427874, 'num_leaves': 147, 'max_depth': 10, 'min_data_in_leaf': 10, 'feature_fraction': 0.7463703458216299, 'bagging_fraction': 0.7752267234812945, 'bagging_freq': 2, 'lambda_l1': 2.6326656752380505e-06, 'lambda_l2': 8.665691856209887e-05}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[286]\tvalid_0's rmse: 0.243532\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.248767\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:35:00,855] Trial 24 finished with value: 0.06087369908815352 and parameters: {'learning_rate': 0.03030951936521498, 'num_leaves': 73, 'max_depth': 11, 'min_data_in_leaf': 24, 'feature_fraction': 0.6597841513597256, 'bagging_fraction': 0.6010447977461073, 'bagging_freq': 3, 'lambda_l1': 0.14688325220926626, 'lambda_l2': 5.692312687087916e-06}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[168]\tvalid_0's rmse: 0.246726\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.241956\n",
            "[400]\tvalid_0's rmse: 0.240481\n",
            "Early stopping, best iteration is:\n",
            "[322]\tvalid_0's rmse: 0.238882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:35:04,449] Trial 25 finished with value: 0.05706478668193402 and parameters: {'learning_rate': 0.01649284414242595, 'num_leaves': 104, 'max_depth': 15, 'min_data_in_leaf': 20, 'feature_fraction': 0.9493707099925303, 'bagging_fraction': 0.6479236074399097, 'bagging_freq': 1, 'lambda_l1': 0.007835084222188947, 'lambda_l2': 0.759390897380507}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.238127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:35:06,697] Trial 26 finished with value: 0.05605909612215715 and parameters: {'learning_rate': 0.02950610068515548, 'num_leaves': 157, 'max_depth': 13, 'min_data_in_leaf': 14, 'feature_fraction': 0.8496920299397761, 'bagging_fraction': 0.7048516260354084, 'bagging_freq': 4, 'lambda_l1': 4.41009404391169e-05, 'lambda_l2': 1.1342894494018081e-07}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[140]\tvalid_0's rmse: 0.236768\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.247005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:35:09,161] Trial 27 finished with value: 0.05982851954586857 and parameters: {'learning_rate': 0.02748070146924849, 'num_leaves': 162, 'max_depth': 14, 'min_data_in_leaf': 15, 'feature_fraction': 0.8748036923054335, 'bagging_fraction': 0.7663145556313192, 'bagging_freq': 6, 'lambda_l1': 6.291899676502158e-05, 'lambda_l2': 1.0013163876898029e-07}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[144]\tvalid_0's rmse: 0.244599\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's rmse: 0.243059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:35:11,852] Trial 28 finished with value: 0.05714235405884513 and parameters: {'learning_rate': 0.0412237101364282, 'num_leaves': 194, 'max_depth': 10, 'min_data_in_leaf': 19, 'feature_fraction': 0.8469949611679061, 'bagging_fraction': 0.8178130258141662, 'bagging_freq': 6, 'lambda_l1': 4.1202276361011383e-07, 'lambda_l2': 5.370542627558154e-07}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping, best iteration is:\n",
            "[135]\tvalid_0's rmse: 0.239045\n",
            "Training until validation scores don't improve for 100 rounds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:35:12,847] Trial 29 finished with value: 0.05828249072993983 and parameters: {'learning_rate': 0.03973961965593817, 'num_leaves': 160, 'max_depth': 8, 'min_data_in_leaf': 25, 'feature_fraction': 0.7504046482943139, 'bagging_fraction': 0.7072454450625336, 'bagging_freq': 4, 'lambda_l1': 6.055133277234501e-06, 'lambda_l2': 4.5806318809058775e-08}. Best is trial 14 with value: 0.05591230701332577.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[200]\tvalid_0's rmse: 0.244653\n",
            "Early stopping, best iteration is:\n",
            "[143]\tvalid_0's rmse: 0.241418\n",
            "Best params: {'learning_rate': 0.013365775645645738, 'num_leaves': 135, 'max_depth': 4, 'min_data_in_leaf': 16, 'feature_fraction': 0.9982114685027492, 'bagging_fraction': 0.6821130233464215, 'bagging_freq': 3, 'lambda_l1': 0.0024075683948095923, 'lambda_l2': 2.864623228001242e-08}\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[200]\tvalid_0's l2: 0.06502\n",
            "Early stopping, best iteration is:\n",
            "[270]\tvalid_0's l2: 0.0621196\n",
            "✅ Validation RMSE: 0.0621, R²: 0.7491\n",
            "✅ Test RMSE: 0.2862, R²: -0.1474\n",
            "💾 Model saved to esol_model.txt\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'exp'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'exp'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1117089126.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Later (new session or notebook)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mESOLTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"exp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"esol_model.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3689257129.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, target_col)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Split X, y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3689257129.py\u001b[0m in \u001b[0;36m_split_xy\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_split_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"smiles\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'exp'"
          ]
        }
      ],
      "source": [
        "trainer = ESOLTrainer(data_dir=\".\", target_col=\"p_np\")\n",
        "\n",
        "# Train with best params\n",
        "model, val_metrics, test_metrics = trainer.trainlgb(tuned=True)\n",
        "\n",
        "# Save it\n",
        "trainer.save_model(\"esol_model.txt\")\n",
        "\n",
        "# Later (new session or notebook)\n",
        "trainer2 = ESOLTrainer(data_dir=\".\", target_col=\"p_np\")\n",
        "loaded_model = trainer2.load_model(\"esol_model.txt\")\n",
        "\n",
        "# Inference on new data\n",
        "new_df = pd.read_csv(\"new_data_with_desc.csv\")\n",
        "X_new = new_df.drop(columns=[\"smiles\", \"exp\"], errors=\"ignore\")\n",
        "preds = loaded_model.predict(X_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdNad-fywtnf"
      },
      "outputs": [],
      "source": [
        "Best params: {'learning_rate': 0.014896238632781483, 'num_leaves': 256, 'max_depth': 11, 'min_data_in_leaf': 29, 'feature_fraction': 0.7642821811125154, 'bagging_fraction': 0.834450819278106, 'bagging_freq': 9, 'lambda_l1': 0.00873099190852502, 'lambda_l2': 0.0013897750455788976}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ont1m0n0uk25",
        "outputId": "447bc761-f4cc-4f12-a62c-9718b83af8a4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4170783825.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Prepare datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ],
      "source": [
        "# Best params from your tuning\n",
        "best_params = {\n",
        "    'learning_rate': 0.014896238632781483,\n",
        "    'num_leaves': 256,\n",
        "    'max_depth': 11,\n",
        "    'min_data_in_leaf': 29,\n",
        "    'feature_fraction': 0.7642821811125154,\n",
        "    'bagging_fraction': 0.834450819278106,\n",
        "    'bagging_freq': 9,\n",
        "    'lambda_l1': 0.00873099190852502,\n",
        "    'lambda_l2': 0.0013897750455788976,\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'verbosity': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Prepare datasets\n",
        "dtrain = lgb.Dataset(trainer.X_train, trainer.y_train)\n",
        "dvalid = lgb.Dataset(trainer.X_valid, trainer.y_valid, reference=dtrain)\n",
        "\n",
        "# Train final model\n",
        "final_model = lgb.train(\n",
        "    best_params,\n",
        "    dtrain,\n",
        "    valid_sets=[dvalid],\n",
        "    num_boost_round=5000,\n",
        "    callbacks=[early_stopping(100), log_evaluation(200)]\n",
        ")\n",
        "\n",
        "# Save\n",
        "trainer.models[\"final\"] = final_model\n",
        "\n",
        "# Evaluate\n",
        "val_preds = final_model.predict(trainer.X_valid, num_iteration=final_model.best_iteration)\n",
        "val_rmse = mean_squared_error(trainer.y_valid, val_preds, squared=False)\n",
        "val_r2 = r2_score(trainer.y_valid, val_preds)\n",
        "\n",
        "test_preds = final_model.predict(trainer.X_test, num_iteration=final_model.best_iteration)\n",
        "test_rmse = mean_squared_error(trainer.y_test, test_preds, squared=False)\n",
        "test_r2 = r2_score(trainer.y_test, test_preds)\n",
        "\n",
        "print(f\"✅ Validation RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
        "print(f\"✅ Test RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pGKWHBBjulWO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import optuna\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "class BBBPExtraTrees:\n",
        "    def __init__(self, data_dir=\".\", target_col=\"p_np\", random_state=42):\n",
        "        \"\"\"\n",
        "        Extra Trees model for BBBP classification with Optuna tuning & persistence.\n",
        "        - Assumes train/valid/test CSVs already prepared with descriptors/fingerprints.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.target_col = target_col\n",
        "        self.random_state = random_state\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "\n",
        "        # Load data\n",
        "        self.train = pd.read_csv(os.path.join(data_dir, \"train_with_desc.csv\"))\n",
        "        self.valid = pd.read_csv(os.path.join(data_dir, \"valid_with_desc.csv\"))\n",
        "        self.test = pd.read_csv(os.path.join(data_dir, \"test_with_desc.csv\"))\n",
        "\n",
        "        # Split X, y\n",
        "        self.X_train, self.y_train = self._split_xy(self.train)\n",
        "        self.X_valid, self.y_valid = self._split_xy(self.valid)\n",
        "        self.X_test, self.y_test = self._split_xy(self.test)\n",
        "\n",
        "    def _split_xy(self, df):\n",
        "        \"\"\"Drop SMILES + target, keep descriptors/fingerprints.\"\"\"\n",
        "        X = df.drop(columns=[self.target_col, \"smiles\"], errors=\"ignore\")\n",
        "        y = df[self.target_col]\n",
        "        return X, y\n",
        "\n",
        "    def _objective(self, trial):\n",
        "        \"\"\"Optuna objective: maximize ROC-AUC on validation set.\"\"\"\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 50),\n",
        "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
        "        }\n",
        "\n",
        "        model = ExtraTreesClassifier(\n",
        "            **params,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "        preds = model.predict_proba(self.X_valid)[:, 1]\n",
        "        return roc_auc_score(self.y_valid, preds)\n",
        "\n",
        "    def tune(self, n_trials=30):\n",
        "        \"\"\"Run Optuna tuning, store best params.\"\"\"\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(self._objective, n_trials=n_trials)\n",
        "\n",
        "        self.best_params = study.best_params\n",
        "        print(\"Best Params:\", self.best_params)\n",
        "        print(\"Best ROC-AUC:\", study.best_value)\n",
        "        return self.best_params\n",
        "\n",
        "    def train_extratrees(self, tuned=True):\n",
        "        \"\"\"Train Extra Trees using either tuned or default params.\"\"\"\n",
        "        if tuned and self.best_params is None:\n",
        "            print(\"⚠️ No tuned params found, running tuning first...\")\n",
        "            self.tune(n_trials=30)\n",
        "\n",
        "        params = self.best_params if (tuned and self.best_params) else {\n",
        "            \"n_estimators\": 300,\n",
        "            \"max_depth\": None,\n",
        "            \"max_features\": \"sqrt\",\n",
        "            \"min_samples_split\": 2,\n",
        "            \"min_samples_leaf\": 1,\n",
        "        }\n",
        "\n",
        "        self.model = ExtraTreesClassifier(\n",
        "            **params,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        # train on train + valid together for final model\n",
        "        X_final = pd.concat([self.X_train, self.X_valid], axis=0)\n",
        "        y_final = pd.concat([self.y_train, self.y_valid], axis=0)\n",
        "        self.model.fit(X_final, y_final)\n",
        "\n",
        "        # Validation metrics\n",
        "        val_preds = self.model.predict(self.X_valid)\n",
        "        val_probs = self.model.predict_proba(self.X_valid)[:, 1]\n",
        "        val_acc = accuracy_score(self.y_valid, val_preds)\n",
        "        val_roc = roc_auc_score(self.y_valid, val_probs)\n",
        "        print(f\"✅ Validation ACC: {val_acc:.4f}, ROC-AUC: {val_roc:.4f}\")\n",
        "\n",
        "        # Test metrics\n",
        "        test_preds = self.model.predict(self.X_test)\n",
        "        test_probs = self.model.predict_proba(self.X_test)[:, 1]\n",
        "        test_acc = accuracy_score(self.y_test, test_preds)\n",
        "        test_roc = roc_auc_score(self.y_test, test_probs)\n",
        "        print(f\"✅ Test ACC: {test_acc:.4f}, ROC-AUC: {test_roc:.4f}\")\n",
        "\n",
        "        return self.model, (val_acc, val_roc), (test_acc, test_roc)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet.\")\n",
        "        return self.model.predict(X_new)\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet.\")\n",
        "        return self.model.predict_proba(X_new)\n",
        "\n",
        "    def save_model(self, filepath=\"bbbp_extratrees.pkl\"):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model trained yet.\")\n",
        "        joblib.dump({\"model\": self.model, \"params\": self.best_params}, filepath)\n",
        "        print(f\"💾 Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath=\"bbbp_extratrees.pkl\"):\n",
        "        data = joblib.load(filepath)\n",
        "        self.model = data[\"model\"]\n",
        "        self.best_params = data.get(\"params\", None)\n",
        "        print(f\"📂 Model loaded from {filepath}\")\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jXQSWuA93ggb"
      },
      "outputs": [],
      "source": [
        "bbbp_trainer = BBBPExtraTrees(data_dir=\".\", target_col=\"p_np\", random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuVLtvZk3lZS",
        "outputId": "d58063ba-3ab3-4870-8d75-276c0fba3392"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:43:15,308] A new study created in memory with name: no-name-8f1419f9-0426-435f-ae2e-25d5db68c688\n",
            "[I 2025-10-07 13:43:16,295] Trial 0 finished with value: 0.9744759316770186 and parameters: {'n_estimators': 466, 'max_depth': 26, 'max_features': 'log2', 'min_samples_split': 13, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:23,396] Trial 1 finished with value: 0.968361801242236 and parameters: {'n_estimators': 538, 'max_depth': 14, 'max_features': None, 'min_samples_split': 19, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:35,087] Trial 2 finished with value: 0.9691381987577639 and parameters: {'n_estimators': 881, 'max_depth': 15, 'max_features': None, 'min_samples_split': 6, 'min_samples_leaf': 8}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:41,762] Trial 3 finished with value: 0.9712732919254659 and parameters: {'n_estimators': 605, 'max_depth': 14, 'max_features': None, 'min_samples_split': 3, 'min_samples_leaf': 15}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:45,732] Trial 4 finished with value: 0.9702057453416149 and parameters: {'n_estimators': 272, 'max_depth': 14, 'max_features': None, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:46,668] Trial 5 finished with value: 0.9705939440993789 and parameters: {'n_estimators': 465, 'max_depth': 26, 'max_features': 'log2', 'min_samples_split': 9, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:47,773] Trial 6 finished with value: 0.9680706521739132 and parameters: {'n_estimators': 504, 'max_depth': 29, 'max_features': 'sqrt', 'min_samples_split': 16, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:48,428] Trial 7 finished with value: 0.9713703416149068 and parameters: {'n_estimators': 392, 'max_depth': 23, 'max_features': 'log2', 'min_samples_split': 12, 'min_samples_leaf': 17}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:50,140] Trial 8 finished with value: 0.969429347826087 and parameters: {'n_estimators': 996, 'max_depth': 38, 'max_features': 'log2', 'min_samples_split': 6, 'min_samples_leaf': 13}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:53,141] Trial 9 finished with value: 0.9712732919254659 and parameters: {'n_estimators': 266, 'max_depth': 27, 'max_features': None, 'min_samples_split': 19, 'min_samples_leaf': 15}. Best is trial 0 with value: 0.9744759316770186.\n",
            "[I 2025-10-07 13:43:53,508] Trial 10 finished with value: 0.9751552795031055 and parameters: {'n_estimators': 129, 'max_depth': 41, 'max_features': 'sqrt', 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.9751552795031055.\n",
            "[I 2025-10-07 13:43:53,934] Trial 11 finished with value: 0.9760287267080745 and parameters: {'n_estimators': 145, 'max_depth': 49, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:43:54,247] Trial 12 finished with value: 0.9757375776397517 and parameters: {'n_estimators': 122, 'max_depth': 46, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:43:54,880] Trial 13 finished with value: 0.9697204968944099 and parameters: {'n_estimators': 167, 'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 16, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:43:55,958] Trial 14 finished with value: 0.9708850931677019 and parameters: {'n_estimators': 283, 'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 17, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:43:57,607] Trial 15 finished with value: 0.9724378881987576 and parameters: {'n_estimators': 671, 'max_depth': 42, 'max_features': 'sqrt', 'min_samples_split': 14, 'min_samples_leaf': 4}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:43:57,935] Trial 16 finished with value: 0.9702057453416149 and parameters: {'n_estimators': 128, 'max_depth': 36, 'max_features': 'sqrt', 'min_samples_split': 20, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:43:59,633] Trial 17 finished with value: 0.9719526397515528 and parameters: {'n_estimators': 733, 'max_depth': 45, 'max_features': 'sqrt', 'min_samples_split': 17, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:00,356] Trial 18 finished with value: 0.968167701863354 and parameters: {'n_estimators': 374, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_split': 11, 'min_samples_leaf': 20}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:00,952] Trial 19 finished with value: 0.9729231366459627 and parameters: {'n_estimators': 214, 'max_depth': 34, 'max_features': 'sqrt', 'min_samples_split': 8, 'min_samples_leaf': 6}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:01,872] Trial 20 finished with value: 0.9753493788819876 and parameters: {'n_estimators': 388, 'max_depth': 46, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:02,843] Trial 21 finished with value: 0.9734083850931676 and parameters: {'n_estimators': 388, 'max_depth': 46, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:03,130] Trial 22 finished with value: 0.9751552795031055 and parameters: {'n_estimators': 101, 'max_depth': 46, 'max_features': 'sqrt', 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:03,683] Trial 23 finished with value: 0.9716614906832299 and parameters: {'n_estimators': 211, 'max_depth': 43, 'max_features': 'sqrt', 'min_samples_split': 18, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:04,503] Trial 24 finished with value: 0.9722437888198758 and parameters: {'n_estimators': 334, 'max_depth': 38, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:04,955] Trial 25 finished with value: 0.970302795031056 and parameters: {'n_estimators': 203, 'max_depth': 32, 'max_features': 'sqrt', 'min_samples_split': 13, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:05,767] Trial 26 finished with value: 0.9715644409937887 and parameters: {'n_estimators': 314, 'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 17, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:06,669] Trial 27 finished with value: 0.9704968944099378 and parameters: {'n_estimators': 224, 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:07,028] Trial 28 finished with value: 0.9743788819875776 and parameters: {'n_estimators': 100, 'max_depth': 47, 'max_features': 'log2', 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.9760287267080745.\n",
            "[I 2025-10-07 13:44:08,299] Trial 29 finished with value: 0.9758346273291925 and parameters: {'n_estimators': 459, 'max_depth': 44, 'max_features': 'sqrt', 'min_samples_split': 11, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.9760287267080745.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Params: {'n_estimators': 145, 'max_depth': 49, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 1}\n",
            "Best ROC-AUC: 0.9760287267080745\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 145,\n",
              " 'max_depth': 49,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_split': 15,\n",
              " 'min_samples_leaf': 1}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bbbp_trainer.tune(n_trials=30)  # can increase n_trials for better search\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uohflbz852si"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def clean_extremes(df, threshold=1e6):\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df = df.clip(-threshold, threshold)\n",
        "    df = df.fillna(df.median())\n",
        "    return df\n",
        "\n",
        "bbbp_trainer.X_train = clean_extremes(bbbp_trainer.X_train)\n",
        "bbbp_trainer.X_valid = clean_extremes(bbbp_trainer.X_valid)\n",
        "bbbp_trainer.X_test  = clean_extremes(bbbp_trainer.X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02ykkML66Gnq",
        "outputId": "0c121ca1-133e-4efc-c650-a338e7f7a1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation ACC: 0.9902, ROC-AUC: 0.9997\n",
            "✅ Test ACC: 0.5686, ROC-AUC: 0.7605\n",
            "Validation metrics (ACC, ROC-AUC): (0.9901960784313726, np.float64(0.9997088509316769))\n",
            "Test metrics (ACC, ROC-AUC): (0.5686274509803921, np.float64(0.760477888043164))\n"
          ]
        }
      ],
      "source": [
        "# Train with tuned params if available\n",
        "model, val_metrics, test_metrics = bbbp_trainer.train_extratrees(tuned=True)\n",
        "print(\"Validation metrics (ACC, ROC-AUC):\", val_metrics)\n",
        "print(\"Test metrics (ACC, ROC-AUC):\", test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M32ynBiB7c9G",
        "outputId": "590b92e2-ff21-471c-ef6e-6d511702e46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Model saved to bbbp_extratrees.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save trained model\n",
        "bbbp_trainer.save_model(\"bbbp_extratrees.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0TGgpvZC7hWV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import optuna\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "class KlipoSVR:\n",
        "    def __init__(self, data_dir=\"data\", target_col=\"logP\", random_state=42):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_col = target_col\n",
        "        self.random_state = random_state\n",
        "        self.scaler = StandardScaler()\n",
        "        self.imputer = SimpleImputer(strategy=\"median\")  # fill NaNs with median\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "\n",
        "        # Load CSVs\n",
        "        self.train = pd.read_csv(os.path.join(data_dir, \"train_with_desc.csv\"))\n",
        "        self.valid = pd.read_csv(os.path.join(data_dir, \"valid_with_desc.csv\"))\n",
        "        self.test = pd.read_csv(os.path.join(data_dir, \"test_with_desc.csv\"))\n",
        "\n",
        "        # Split X, y\n",
        "        self.X_train, self.y_train = self._split_xy(self.train)\n",
        "        self.X_valid, self.y_valid = self._split_xy(self.valid)\n",
        "        self.X_test, self.y_test = self._split_xy(self.test)\n",
        "\n",
        "        # Impute missing values\n",
        "        self.X_train = self.imputer.fit_transform(self.X_train)\n",
        "        self.X_valid = self.imputer.transform(self.X_valid)\n",
        "        self.X_test = self.imputer.transform(self.X_test)\n",
        "\n",
        "        # Scale features\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_valid = self.scaler.transform(self.X_valid)\n",
        "        self.X_test = self.scaler.transform(self.X_test)\n",
        "\n",
        "    def _split_xy(self, df):\n",
        "        \"\"\"Drop SMILES + target, keep descriptors only\"\"\"\n",
        "        X = df.drop(columns=[self.target_col, \"smiles\"], errors=\"ignore\")\n",
        "        y = df[self.target_col]\n",
        "        return X, y\n",
        "\n",
        "    def _objective(self, trial):\n",
        "        \"\"\"Optuna objective: minimize RMSE on validation set\"\"\"\n",
        "        params = {\n",
        "            \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
        "            \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
        "            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
        "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"rbf\", \"poly\", \"sigmoid\"]),\n",
        "        }\n",
        "\n",
        "        model = SVR(**params)\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "        preds = model.predict(self.X_valid)\n",
        "        rmse = np.sqrt(mean_squared_error(self.y_valid, preds))\n",
        "        return rmse\n",
        "\n",
        "    def tune(self, n_trials=50):\n",
        "        \"\"\"Run Optuna tuning\"\"\"\n",
        "        study = optuna.create_study(direction=\"minimize\")\n",
        "        study.optimize(self._objective, n_trials=n_trials)\n",
        "        self.best_params = study.best_params\n",
        "        print(\"Best Params:\", self.best_params)\n",
        "        print(\"Best RMSE:\", study.best_value)\n",
        "        return self.best_params\n",
        "\n",
        "    def train_svr(self, tuned=True):\n",
        "        \"\"\"Train SVR using tuned or default params\"\"\"\n",
        "        if tuned and self.best_params is None:\n",
        "            print(\"⚠️ No tuned params found, running tuning first...\")\n",
        "            self.tune(n_trials=50)\n",
        "\n",
        "        params = self.best_params if (tuned and self.best_params) else {\n",
        "            \"C\": 1.0,\n",
        "            \"epsilon\": 0.1,\n",
        "            \"gamma\": \"scale\",\n",
        "            \"kernel\": \"rbf\"\n",
        "        }\n",
        "\n",
        "        self.model = SVR(**params)\n",
        "        X_final = pd.concat([pd.DataFrame(self.X_train), pd.DataFrame(self.X_valid)], axis=0).values\n",
        "        y_final = pd.concat([self.y_train, self.y_valid], axis=0).values\n",
        "        self.model.fit(X_final, y_final)\n",
        "\n",
        "        # Validation metrics\n",
        "        val_preds = self.model.predict(self.X_valid)\n",
        "        val_rmse = np.sqrt(mean_squared_error(self.y_valid, val_preds))\n",
        "        val_r2 = r2_score(self.y_valid, val_preds)\n",
        "        print(f\"✅ Validation RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
        "\n",
        "        # Test metrics\n",
        "        test_preds = self.model.predict(self.X_test)\n",
        "        test_rmse = np.sqrt(mean_squared_error(self.y_valid, test_preds))\n",
        "        test_r2 = r2_score(self.y_test, test_preds)\n",
        "        print(f\"✅ Test RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
        "\n",
        "        return self.model, (val_rmse, val_r2), (test_rmse, test_r2)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        X_new_scaled = self.scaler.transform(X_new)\n",
        "        return self.model.predict(X_new_scaled)\n",
        "\n",
        "    def save_model(self, filepath=\"klipo_svr.pkl\"):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model trained yet.\")\n",
        "        joblib.dump({\"model\": self.model, \"scaler\": self.scaler, \"params\": self.best_params}, filepath)\n",
        "        print(f\"💾 Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath=\"klipo_svr.pkl\"):\n",
        "        data = joblib.load(filepath)\n",
        "        self.model = data[\"model\"]\n",
        "        self.scaler = data[\"scaler\"]\n",
        "        self.best_params = data.get(\"params\", None)\n",
        "        print(f\"📂 Model loaded from {filepath}\")\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOAH3rkOAtbV",
        "outputId": "2c671150-0669-4d82-9bba-87d44334d446"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 14:19:44,877] A new study created in memory with name: no-name-af317a38-29d7-4714-897d-81da708bdf3a\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:47,289] Trial 0 finished with value: 0.6518016585983064 and parameters: {'C': 57.14793390273132, 'epsilon': 0.023664596154148884, 'gamma': 'scale', 'kernel': 'rbf'}. Best is trial 0 with value: 0.6518016585983064.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:49,931] Trial 1 finished with value: 11.25532250368084 and parameters: {'C': 94.2562150612123, 'epsilon': 0.03685804451659977, 'gamma': 'auto', 'kernel': 'poly'}. Best is trial 0 with value: 0.6518016585983064.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:50,273] Trial 2 finished with value: 0.8601725617397932 and parameters: {'C': 0.018855927168823464, 'epsilon': 0.858372216709016, 'gamma': 'auto', 'kernel': 'sigmoid'}. Best is trial 0 with value: 0.6518016585983064.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:51,209] Trial 3 finished with value: 0.628596788606792 and parameters: {'C': 13.727440846721821, 'epsilon': 0.22797329445639422, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:53,698] Trial 4 finished with value: 0.6415720062184848 and parameters: {'C': 33.424590592609725, 'epsilon': 0.00330487897918573, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:54,052] Trial 5 finished with value: 0.6551903284998636 and parameters: {'C': 6.6401716132542665, 'epsilon': 0.544847338686116, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:54,724] Trial 6 finished with value: 2.3871138983124247 and parameters: {'C': 0.036485770803916975, 'epsilon': 0.0648302929060846, 'gamma': 'scale', 'kernel': 'poly'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:55,448] Trial 7 finished with value: 5.690783658724089 and parameters: {'C': 0.17854911680569416, 'epsilon': 0.004401525758346742, 'gamma': 'scale', 'kernel': 'poly'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:56,484] Trial 8 finished with value: 444.31292958512824 and parameters: {'C': 14.095563327936452, 'epsilon': 0.6961065054690367, 'gamma': 'auto', 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:58,175] Trial 9 finished with value: 0.6426507997962736 and parameters: {'C': 170.96403885423405, 'epsilon': 0.08018806128399526, 'gamma': 'scale', 'kernel': 'rbf'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:19:58,906] Trial 10 finished with value: 0.6426448446521009 and parameters: {'C': 0.825903300703753, 'epsilon': 0.1884451834853985, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:02,600] Trial 11 finished with value: 0.6634861261462828 and parameters: {'C': 883.7032025823728, 'epsilon': 0.0016662942387091902, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 3 with value: 0.628596788606792.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:03,850] Trial 12 finished with value: 0.6198602361629852 and parameters: {'C': 1.9792410047307802, 'epsilon': 0.010828066838795515, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 12 with value: 0.6198602361629852.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:05,010] Trial 13 finished with value: 0.6249434034635826 and parameters: {'C': 1.4754042085178218, 'epsilon': 0.012128829282747493, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 12 with value: 0.6198602361629852.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:06,100] Trial 14 finished with value: 0.6367133786826974 and parameters: {'C': 0.913348486519717, 'epsilon': 0.011850315474946456, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 12 with value: 0.6198602361629852.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:07,134] Trial 15 finished with value: 25.686306427880584 and parameters: {'C': 0.8238820100677302, 'epsilon': 0.00920451466906497, 'gamma': 'auto', 'kernel': 'sigmoid'}. Best is trial 12 with value: 0.6198602361629852.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:08,488] Trial 16 finished with value: 0.6181263147148874 and parameters: {'C': 2.631454426892058, 'epsilon': 0.012308759548259534, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:09,538] Trial 17 finished with value: 0.707196188046083 and parameters: {'C': 0.18517039859136972, 'epsilon': 0.004472059977556468, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:10,292] Trial 18 finished with value: 5.71342148638322 and parameters: {'C': 0.18027034552884894, 'epsilon': 0.0010894567333641528, 'gamma': 'scale', 'kernel': 'poly'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:11,570] Trial 19 finished with value: 136.86266999498486 and parameters: {'C': 4.324936924627487, 'epsilon': 0.027299720340211997, 'gamma': 'auto', 'kernel': 'sigmoid'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:12,970] Trial 20 finished with value: 0.6190087138746192 and parameters: {'C': 2.135067727264995, 'epsilon': 0.008361770612894623, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:14,264] Trial 21 finished with value: 0.6183489781337779 and parameters: {'C': 2.33485653680752, 'epsilon': 0.008562932527563667, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:15,310] Trial 22 finished with value: 0.680712744079545 and parameters: {'C': 0.2978417750369211, 'epsilon': 0.0064252005577018914, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:16,917] Trial 23 finished with value: 0.6192488228074474 and parameters: {'C': 4.250746777862702, 'epsilon': 0.0021984377639210216, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:18,878] Trial 24 finished with value: 0.6317172658408543 and parameters: {'C': 12.384973467251102, 'epsilon': 0.006847247202058446, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:19,907] Trial 25 finished with value: 0.6732388548044826 and parameters: {'C': 0.34756671140743955, 'epsilon': 0.018389859849566956, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:20,886] Trial 26 finished with value: 0.7844179899218062 and parameters: {'C': 0.06558849839290544, 'epsilon': 0.042424572675573896, 'gamma': 'scale', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:22,161] Trial 27 finished with value: 0.6188840401020451 and parameters: {'C': 2.1946276383321077, 'epsilon': 0.01712603321249835, 'gamma': 'auto', 'kernel': 'rbf'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:24,541] Trial 28 finished with value: 11.377262138829707 and parameters: {'C': 25.104557033835835, 'epsilon': 0.01591648734984441, 'gamma': 'auto', 'kernel': 'poly'}. Best is trial 16 with value: 0.6181263147148874.\n",
            "/tmp/ipython-input-2689048111.py:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"C\": trial.suggest_loguniform(\"C\", 1e-2, 1e3),\n",
            "/tmp/ipython-input-2689048111.py:51: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  \"epsilon\": trial.suggest_loguniform(\"epsilon\", 1e-3, 1.0),\n",
            "[I 2025-10-07 14:20:25,589] Trial 29 finished with value: 239.85848332749455 and parameters: {'C': 7.219979456278356, 'epsilon': 0.062126808944539556, 'gamma': 'scale', 'kernel': 'sigmoid'}. Best is trial 16 with value: 0.6181263147148874.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Params: {'C': 2.631454426892058, 'epsilon': 0.012308759548259534, 'gamma': 'auto', 'kernel': 'rbf'}\n",
            "Best RMSE: 0.6181263147148874\n",
            "✅ Validation RMSE: 0.2436, R²: 0.9413\n",
            "✅ Test RMSE: 1.2590, R²: 0.6046\n",
            "💾 Model saved to klipo_svr.pkl\n"
          ]
        }
      ],
      "source": [
        "klipo = KlipoSVR(data_dir=\".\", target_col=\"exp\")\n",
        "\n",
        "klipo.tune(n_trials=30)  # reduce/increase n_trials as needed\n",
        "\n",
        "klipo.train_svr(tuned=True)\n",
        "klipo.save_model(\"klipo_svr.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "A7HR8o_-A-TN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from src.featuriser import DescriptorFeaturizer\n",
        "import optuna\n",
        "\n",
        "class KlipoSKLearn:\n",
        "    def __init__(self, data_dir=\"data\", target_col=\"logP\", random_state=42):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_col = target_col\n",
        "        self.random_state = random_state\n",
        "        self.imputer = SimpleImputer(strategy=\"median\")\n",
        "        self.featurizer = DescriptorFeaturizer()\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "\n",
        "        # Load raw CSVs\n",
        "        self.train = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "        self.valid = pd.read_csv(os.path.join(data_dir, \"valid.csv\"))\n",
        "        self.test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "        # Featurize descriptors\n",
        "        self.X_train = self.featurize_df(self.train)\n",
        "        self.X_valid = self.featurize_df(self.valid)\n",
        "        self.X_test = self.featurize_df(self.test)\n",
        "\n",
        "        self.y_train = self.train[self.target_col].values\n",
        "        self.y_valid = self.valid[self.target_col].values\n",
        "        self.y_test = self.test[self.target_col].values\n",
        "\n",
        "        # Impute missing values\n",
        "        self.X_train = self.imputer.fit_transform(self.X_train)\n",
        "        self.X_valid = self.imputer.transform(self.X_valid)\n",
        "        self.X_test = self.imputer.transform(self.X_test)\n",
        "\n",
        "    def featurize_df(self, df):\n",
        "        \"\"\"Featurize a dataframe of SMILES\"\"\"\n",
        "        features = []\n",
        "        for smi in df[\"smiles\"]:\n",
        "            desc = self.featurizer.featurize_smiles(smi)\n",
        "            features.append(desc)\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    def _objective(self, trial):\n",
        "        \"\"\"Optuna objective to minimize RMSE\"\"\"\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "            \"random_state\": self.random_state,\n",
        "        }\n",
        "\n",
        "        model = GradientBoostingRegressor(**params)\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "        preds = model.predict(self.X_valid)\n",
        "        rmse = np.sqrt(mean_squared_error(self.y_valid, preds))\n",
        "        return rmse\n",
        "\n",
        "    def tune(self, n_trials=30):\n",
        "        \"\"\"Run Optuna hyperparameter tuning\"\"\"\n",
        "        study = optuna.create_study(direction=\"minimize\")\n",
        "        study.optimize(self._objective, n_trials=n_trials)\n",
        "        self.best_params = study.best_params\n",
        "        print(\"✅ Best Params:\", self.best_params)\n",
        "        print(\"✅ Best RMSE:\", study.best_value)\n",
        "        return self.best_params\n",
        "\n",
        "    def p_train(self, tuned=True):\n",
        "        \"\"\"Train final model on train + valid\"\"\"\n",
        "        if tuned and self.best_params is None:\n",
        "            print(\"⚠️ No tuned params found, running tuning first...\")\n",
        "            self.tune(n_trials=30)\n",
        "\n",
        "        params = self.best_params if (tuned and self.best_params) else {\n",
        "            \"n_estimators\": 500,\n",
        "            \"max_depth\": 6,\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"subsample\": 0.8,\n",
        "            \"random_state\": self.random_state,\n",
        "        }\n",
        "\n",
        "        self.model = GradientBoostingRegressor(**params)\n",
        "        X_full = np.vstack([self.X_train, self.X_valid])\n",
        "        y_full = np.concatenate([self.y_train, self.y_valid])\n",
        "        self.model.fit(X_full, y_full)\n",
        "\n",
        "        # Metrics\n",
        "        val_preds = self.model.predict(self.X_valid)\n",
        "        val_rmse = np.sqrt(mean_squared_error(self.y_valid, val_preds))\n",
        "        val_r2 = r2_score(self.y_valid, val_preds)\n",
        "        print(f\"✅ Validation RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
        "\n",
        "        test_preds = self.model.predict(self.X_test)\n",
        "        test_rmse = np.sqrt(mean_squared_error(self.y_test, test_preds))\n",
        "        test_r2 = r2_score(self.y_test, test_preds)\n",
        "        print(f\"✅ Test RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
        "\n",
        "        return self.model, (val_rmse, val_r2), (test_rmse, test_r2)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predict on new featurized data\"\"\"\n",
        "        return self.model.predict(X_new)\n",
        "\n",
        "    def save_model(self, filepath=\"./models/klipo_skl.pkl\"):\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        joblib.dump({\"model\": self.model, \"imputer\": self.imputer}, filepath)\n",
        "        print(f\"💾 Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath=\"./models/klipo_skl.pkl\"):\n",
        "        data = joblib.load(filepath)\n",
        "        self.model = data[\"model\"]\n",
        "        self.imputer = data[\"imputer\"]\n",
        "        print(f\"📂 Model loaded from {filepath}\")\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 18:55:48,214 - INFO - DescriptorFeaturizer initialized with 217 descriptors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Validation RMSE: 0.1060, R²: 0.9889\n",
            "✅ Test RMSE: 0.5718, R²: 0.6065\n",
            "✅ Validation RMSE: 0.1060, R²: 0.9889\n",
            "✅ Test RMSE: 0.5718, R²: 0.6065\n",
            "💾 Model saved to ./models/klipo_skl_best.pkl\n"
          ]
        }
      ],
      "source": [
        "trainer = KlipoSKLearn(\n",
        "    data_dir=\"data/lipo\",\n",
        "    target_col=\"target\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train final model with best parameters\n",
        "best_params = {\n",
        "    'n_estimators': 872,\n",
        "    'max_depth': 5,\n",
        "    'learning_rate': 0.13318585500919636,\n",
        "    'subsample': 0.68744377756399,\n",
        "    'min_samples_split': 6,\n",
        "    'min_samples_leaf': 7,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "trainer.p_train(tuned=False)\n",
        "trainer.model.set_params(**best_params)\n",
        "trainer.p_train(tuned=False)\n",
        "trainer.save_model(filepath=\"./models/klipo_skl_best.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from src.featuriser import DescriptorFeaturizer\n",
        "\n",
        "class BBBPExtraTrees:\n",
        "    def __init__(self, data_dir=\"data/bbbp\", target_col=\"p_np\", random_state=42):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_col = target_col\n",
        "        self.random_state = random_state\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.featurizer = DescriptorFeaturizer()\n",
        "\n",
        "        # Load raw CSVs\n",
        "        self.train = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
        "        self.valid = pd.read_csv(os.path.join(data_dir, \"valid.csv\"))\n",
        "        self.test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
        "\n",
        "        # Featurize descriptors\n",
        "        self.X_train = self.featurize_df(self.train)\n",
        "        self.X_valid = self.featurize_df(self.valid)\n",
        "        self.X_test = self.featurize_df(self.test)\n",
        "\n",
        "        # Normalise / scale descriptors\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_valid = self.scaler.transform(self.X_valid)\n",
        "        self.X_test = self.scaler.transform(self.X_test)\n",
        "\n",
        "        # Targets\n",
        "        self.y_train = self.train[self.target_col].values\n",
        "        self.y_valid = self.valid[self.target_col].values\n",
        "        self.y_test = self.test[self.target_col].values\n",
        "\n",
        "        # Compute class weights for imbalance\n",
        "        classes = np.unique(self.y_train)\n",
        "        self.class_weights = dict(zip(\n",
        "            classes,\n",
        "            compute_class_weight(class_weight=\"balanced\", classes=classes, y=self.y_train)\n",
        "        ))\n",
        "\n",
        "    def featurize_df(self, df):\n",
        "        \"\"\"Generate descriptors from SMILES and handle large/infinite values\"\"\"\n",
        "        features = []\n",
        "        for smi in df[\"smiles\"]:\n",
        "            desc = self.featurizer.featurize_smiles(smi)\n",
        "            desc = np.nan_to_num(desc, nan=0.0, posinf=np.finfo(np.float32).max, neginf=-np.finfo(np.float32).max)\n",
        "            features.append(desc)\n",
        "        X = np.array(features, dtype=np.float32)\n",
        "\n",
        "        # Replace extremely large values with median of that column\n",
        "        for i in range(X.shape[1]):\n",
        "            col = X[:, i]\n",
        "            max_val = np.finfo(np.float32).max\n",
        "            median_val = np.median(col[np.isfinite(col)])\n",
        "            col[col > max_val] = median_val\n",
        "            col[col < -max_val] = median_val\n",
        "            X[:, i] = col\n",
        "        return X\n",
        "\n",
        "    def _objective(self, trial):\n",
        "        \"\"\"Optuna objective: maximize ROC-AUC on validation set\"\"\"\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 50),\n",
        "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
        "        }\n",
        "\n",
        "        model = ExtraTreesClassifier(\n",
        "            **params,\n",
        "            random_state=self.random_state,\n",
        "            class_weight=self.class_weights,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "        preds = model.predict_proba(self.X_valid)[:, 1]\n",
        "        return roc_auc_score(self.y_valid, preds)\n",
        "\n",
        "    def tune(self, n_trials=30):\n",
        "        \"\"\"Run Optuna tuning, store best params.\"\"\"\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(self._objective, n_trials=n_trials)\n",
        "        self.best_params = study.best_params\n",
        "        print(\"✅ Best Params:\", self.best_params)\n",
        "        print(\"✅ Best ROC-AUC:\", study.best_value)\n",
        "        return self.best_params\n",
        "\n",
        "    def train_extratrees(self, tuned=True):\n",
        "        \"\"\"Train Extra Trees using tuned or default params\"\"\"\n",
        "        if tuned and self.best_params is None:\n",
        "            print(\"⚠️ No tuned params found, running tuning first...\")\n",
        "            self.tune(n_trials=30)\n",
        "\n",
        "        params = self.best_params if (tuned and self.best_params) else {\n",
        "            \"n_estimators\": 300,\n",
        "            \"max_depth\": None,\n",
        "            \"max_features\": \"sqrt\",\n",
        "            \"min_samples_split\": 2,\n",
        "            \"min_samples_leaf\": 1,\n",
        "        }\n",
        "\n",
        "        self.model = ExtraTreesClassifier(\n",
        "            **params,\n",
        "            random_state=self.random_state,\n",
        "            class_weight=self.class_weights,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        # Train on train + valid together for final model\n",
        "        X_final = np.vstack([self.X_train, self.X_valid])\n",
        "        y_final = np.concatenate([self.y_train, self.y_valid])\n",
        "        self.model.fit(X_final, y_final)\n",
        "\n",
        "        # Validation metrics\n",
        "        val_preds = self.model.predict(self.X_valid)\n",
        "        val_probs = self.model.predict_proba(self.X_valid)[:, 1]\n",
        "        val_acc = accuracy_score(self.y_valid, val_preds)\n",
        "        val_roc = roc_auc_score(self.y_valid, val_probs)\n",
        "        print(f\"✅ Validation ACC: {val_acc:.4f}, ROC-AUC: {val_roc:.4f}\")\n",
        "\n",
        "        # Test metrics\n",
        "        test_preds = self.model.predict(self.X_test)\n",
        "        test_probs = self.model.predict_proba(self.X_test)[:, 1]\n",
        "        test_acc = accuracy_score(self.y_test, test_preds)\n",
        "        test_roc = roc_auc_score(self.y_test, test_probs)\n",
        "        print(f\"✅ Test ACC: {test_acc:.4f}, ROC-AUC: {test_roc:.4f}\")\n",
        "\n",
        "        return self.model, (val_acc, val_roc), (test_acc, test_roc)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predict class labels from featurized descriptors\"\"\"\n",
        "        X_new_scaled = self.scaler.transform(X_new)\n",
        "        return self.model.predict(X_new_scaled)\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        X_new_scaled = self.scaler.transform(X_new)\n",
        "        return self.model.predict_proba(X_new_scaled)\n",
        "\n",
        "    def save_model(self, filepath=\"./models/bbbp_extratrees.pkl\"):\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model trained yet.\")\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        joblib.dump({\n",
        "            \"model\": self.model,\n",
        "            \"scaler\": self.scaler,\n",
        "            \"best_params\": self.best_params\n",
        "        }, filepath)\n",
        "        print(f\"💾 Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath=\"./models/bbbp_extratrees.pkl\"):\n",
        "        data = joblib.load(filepath)\n",
        "        self.model = data[\"model\"]\n",
        "        self.scaler = data[\"scaler\"]\n",
        "        self.best_params = data.get(\"best_params\", None)\n",
        "        print(f\"📂 Model loaded from {filepath}\")\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-08 19:14:00,264 - INFO - DescriptorFeaturizer initialized with 217 descriptors\n",
            "[19:14:00] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:00] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:00] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:00] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:02] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:02] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:02] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:02] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:03] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:05] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:06] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:06] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:07] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:08] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:08] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:08] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:08] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:08] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:08] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:09] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:10] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:11] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:11] WARNING: not removing hydrogen atom without neighbors\n",
            "/var/folders/gw/1l5px9bx4rj26hgrvvwkkhf40000gn/T/ipykernel_860/4260357829.py:56: RuntimeWarning: overflow encountered in cast\n",
            "  X = np.array(features, dtype=np.float32)\n",
            "[19:14:13] WARNING: not removing hydrogen atom without neighbors\n",
            "[19:14:13] WARNING: not removing hydrogen atom without neighbors\n",
            "[I 2025-10-08 19:14:13,620] A new study created in memory with name: no-name-eb7d183f-9d01-4a2b-8c12-421c490ae9c9\n",
            "[I 2025-10-08 19:14:14,036] Trial 0 finished with value: 0.970496894409938 and parameters: {'n_estimators': 640, 'max_depth': 24, 'max_features': 'log2', 'min_samples_split': 13, 'min_samples_leaf': 16}. Best is trial 0 with value: 0.970496894409938.\n",
            "[I 2025-10-08 19:14:14,405] Trial 1 finished with value: 0.9736024844720497 and parameters: {'n_estimators': 192, 'max_depth': 29, 'max_features': None, 'min_samples_split': 7, 'min_samples_leaf': 13}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:14,940] Trial 2 finished with value: 0.9716614906832299 and parameters: {'n_estimators': 868, 'max_depth': 43, 'max_features': 'sqrt', 'min_samples_split': 17, 'min_samples_leaf': 20}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:15,435] Trial 3 finished with value: 0.9718555900621119 and parameters: {'n_estimators': 751, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_split': 12, 'min_samples_leaf': 14}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:15,649] Trial 4 finished with value: 0.9719526397515528 and parameters: {'n_estimators': 101, 'max_depth': 30, 'max_features': None, 'min_samples_split': 20, 'min_samples_leaf': 20}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:16,027] Trial 5 finished with value: 0.9726319875776398 and parameters: {'n_estimators': 521, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:16,509] Trial 6 finished with value: 0.9717585403726708 and parameters: {'n_estimators': 319, 'max_depth': 44, 'max_features': 'sqrt', 'min_samples_split': 8, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:16,626] Trial 7 finished with value: 0.9708850931677019 and parameters: {'n_estimators': 153, 'max_depth': 26, 'max_features': 'log2', 'min_samples_split': 7, 'min_samples_leaf': 13}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:17,109] Trial 8 finished with value: 0.9732142857142858 and parameters: {'n_estimators': 699, 'max_depth': 24, 'max_features': 'sqrt', 'min_samples_split': 9, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:17,526] Trial 9 finished with value: 0.9735054347826088 and parameters: {'n_estimators': 666, 'max_depth': 46, 'max_features': 'log2', 'min_samples_split': 15, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:18,086] Trial 10 finished with value: 0.9676824534161491 and parameters: {'n_estimators': 396, 'max_depth': 6, 'max_features': None, 'min_samples_split': 3, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.9736024844720497.\n",
            "[I 2025-10-08 19:14:18,713] Trial 11 finished with value: 0.9749611801242235 and parameters: {'n_estimators': 978, 'max_depth': 37, 'max_features': 'log2', 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:21,365] Trial 12 finished with value: 0.9719526397515529 and parameters: {'n_estimators': 974, 'max_depth': 34, 'max_features': None, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:22,156] Trial 13 finished with value: 0.9736024844720497 and parameters: {'n_estimators': 290, 'max_depth': 40, 'max_features': None, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:22,477] Trial 14 finished with value: 0.969526397515528 and parameters: {'n_estimators': 507, 'max_depth': 17, 'max_features': 'log2', 'min_samples_split': 5, 'min_samples_leaf': 16}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:24,505] Trial 15 finished with value: 0.9722437888198758 and parameters: {'n_estimators': 988, 'max_depth': 50, 'max_features': None, 'min_samples_split': 10, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:25,003] Trial 16 finished with value: 0.9717585403726708 and parameters: {'n_estimators': 838, 'max_depth': 35, 'max_features': 'log2', 'min_samples_split': 5, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:25,142] Trial 17 finished with value: 0.9737965838509317 and parameters: {'n_estimators': 218, 'max_depth': 19, 'max_features': 'log2', 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:25,417] Trial 18 finished with value: 0.9719526397515529 and parameters: {'n_estimators': 420, 'max_depth': 17, 'max_features': 'log2', 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:25,586] Trial 19 finished with value: 0.9682647515527951 and parameters: {'n_estimators': 256, 'max_depth': 7, 'max_features': 'log2', 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:25,944] Trial 20 finished with value: 0.969817546583851 and parameters: {'n_estimators': 587, 'max_depth': 11, 'max_features': 'log2', 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:26,374] Trial 21 finished with value: 0.971758540372671 and parameters: {'n_estimators': 214, 'max_depth': 21, 'max_features': None, 'min_samples_split': 7, 'min_samples_leaf': 11}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:26,607] Trial 22 finished with value: 0.969526397515528 and parameters: {'n_estimators': 382, 'max_depth': 31, 'max_features': 'log2', 'min_samples_split': 10, 'min_samples_leaf': 16}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:26,737] Trial 23 finished with value: 0.9719526397515529 and parameters: {'n_estimators': 182, 'max_depth': 40, 'max_features': 'log2', 'min_samples_split': 6, 'min_samples_leaf': 11}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:27,011] Trial 24 finished with value: 0.9714673913043479 and parameters: {'n_estimators': 108, 'max_depth': 21, 'max_features': None, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:27,283] Trial 25 finished with value: 0.9733113354037267 and parameters: {'n_estimators': 445, 'max_depth': 30, 'max_features': 'log2', 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:27,880] Trial 26 finished with value: 0.9725349378881988 and parameters: {'n_estimators': 334, 'max_depth': 38, 'max_features': None, 'min_samples_split': 8, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:28,046] Trial 27 finished with value: 0.9709821428571429 and parameters: {'n_estimators': 232, 'max_depth': 27, 'max_features': 'log2', 'min_samples_split': 6, 'min_samples_leaf': 18}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:28,525] Trial 28 finished with value: 0.9700116459627329 and parameters: {'n_estimators': 806, 'max_depth': 21, 'max_features': 'log2', 'min_samples_split': 11, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.9749611801242235.\n",
            "[I 2025-10-08 19:14:29,833] Trial 29 finished with value: 0.9762228260869564 and parameters: {'n_estimators': 484, 'max_depth': 32, 'max_features': None, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 29 with value: 0.9762228260869564.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Best Params: {'n_estimators': 484, 'max_depth': 32, 'max_features': None, 'min_samples_split': 3, 'min_samples_leaf': 3}\n",
            "✅ Best ROC-AUC: 0.9762228260869564\n",
            "Tuned parameters: {'n_estimators': 484, 'max_depth': 32, 'max_features': None, 'min_samples_split': 3, 'min_samples_leaf': 3}\n",
            "✅ Validation ACC: 0.9951, ROC-AUC: 1.0000\n",
            "✅ Test ACC: 0.6029, ROC-AUC: 0.7454\n",
            "Validation Metrics (ACC, ROC-AUC): (0.9950980392156863, 1.0)\n",
            "Test Metrics (ACC, ROC-AUC): (0.6029411764705882, 0.7453511899026881)\n",
            "💾 Model saved to ./models/bbbp_extratrees_best.pkl\n"
          ]
        }
      ],
      "source": [
        "trainer = BBBPExtraTrees(\n",
        "    data_dir=\"data/bbbp\",   # Folder containing train.csv, valid.csv, test.csv\n",
        "    target_col=\"p_np\",      # Column name for BBB permeability\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# %% [code]\n",
        "# Step 3: (Optional) Hyperparameter tuning using Optuna\n",
        "# This may take a while. If you want to skip, set tuned=False in train_extratrees.\n",
        "tuned_params = trainer.tune(n_trials=30)\n",
        "print(\"Tuned parameters:\", tuned_params)\n",
        "\n",
        "# %% [code]\n",
        "# Step 4: Train final ExtraTrees model (using tuned parameters)\n",
        "model, val_metrics, test_metrics = trainer.train_extratrees(tuned=True)\n",
        "print(\"Validation Metrics (ACC, ROC-AUC):\", val_metrics)\n",
        "print(\"Test Metrics (ACC, ROC-AUC):\", test_metrics)\n",
        "\n",
        "# %% [code]\n",
        "# Step 5: Save the trained model\n",
        "trainer.save_model(filepath=\"./models/bbbp_extratrees_best.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".phenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
